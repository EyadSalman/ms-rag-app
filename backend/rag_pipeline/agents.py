# backend/rag_pipeline/agents.py

import os
os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0"
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
from langchain_core.documents import Document
from langchain_community.vectorstores import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_huggingface import HuggingFaceEmbeddings

#  Initialize LLM and embeddings
embedding = HuggingFaceEmbeddings(model_name="mixedbread-ai/mxbai-embed-large-v1")
llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.1)


# Load vectorstores
MRI_DB = Chroma(persist_directory="vectorstores/mri_db_free", embedding_function=embedding)
RESEARCH_DB = Chroma(persist_directory="vectorstores/research_db_free", embedding_function=embedding)

def mri_agent(query: str):
    """Retrieve MRI examples similar to a query."""
    results = MRI_DB.similarity_search(query, k=3)
    context = "\n".join([r.page_content for r in results]) or "No MRI examples found."
    prompt = f"You are an MRI interpretation assistant.\n\n{context}\n\nQuestion: {query}\nAnswer:"
    resp = llm.invoke(prompt)
    print("Gemini MRI response raw:", resp)
    return {
  "answer": resp.content,
  "agent_type": "research",
  "rag_used": len(results) > 0,
  "sources": [r.metadata.get("source") for r in results]
}



def research_agent(query: str):
    """Retrieve relevant medical papers."""
    results = RESEARCH_DB.similarity_search(query, k=3)
    context = "\n".join([r.page_content for r in results]) or "No relevant papers found."
    prompt = f"""
You are a medical research assistant specialized in Multiple Sclerosis (MS).

Use the information from the research context below to answer the user's question.
If the context is insufficient, say so and do not make up information.

Format your answer as:
1. A concise summary (2â€“3 sentences)
2. Supporting evidence or study details (with source names or authors) or experiement results
3. A closing statement if applicable.

### Research Context:
{context}

### Question:
{query}

### Answer:
"""

    resp = llm.invoke(prompt)
    print("Gemini Research response raw:", resp)
    # If Gemini hit token limit or gave empty content, retry shorter
    if not resp.content.strip():
        short_prompt = f"Answer briefly: {query}"
        retry = llm.invoke(short_prompt)
        return retry.content or "No response generated by Gemini."
    return {
  "answer": resp.content,
  "agent_type": "research",
  "rag_used": len(results) > 0,
  "sources": [r.metadata.get("source") for r in results]
}


