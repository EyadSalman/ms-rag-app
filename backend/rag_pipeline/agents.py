# backend/rag_pipeline/agents.py

import os
os.environ["TF_ENABLE_ONEDNN_OPTS"] = "0"
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"
from langchain_core.documents import Document
from langchain_community.vectorstores import Chroma
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_huggingface import HuggingFaceEmbeddings

#  Initialize LLM and embeddings
embedding = HuggingFaceEmbeddings(model_name="mixedbread-ai/mxbai-embed-large-v1")
llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.1)


# Load vectorstores
MRI_DB = Chroma(persist_directory="vectorstores/mri_db_free", embedding_function=embedding)
RESEARCH_DB = Chroma(persist_directory="vectorstores/research_db_free", embedding_function=embedding)

def mri_agent(query: str):
    """Retrieve MRI examples similar to a query."""
    results = MRI_DB.similarity_search(query, k=3)
    context = "\n".join([r.page_content for r in results]) or "No MRI examples found."
    prompt = f"You are an MRI interpretation assistant.\n\n{context}\n\nQuestion: {query}\nAnswer:"
    resp = llm.invoke(prompt)
    print("Gemini MRI response raw:", resp)
    return {
  "answer": resp.content,
  "agent_type": "research",
  "rag_used": len(results) > 0,
  "sources": [r.metadata.get("source") for r in results]
}



def research_agent(query: str):
    """Retrieve relevant medical papers and generate a clear final answer."""
    results = RESEARCH_DB.similarity_search(query, k=3)
    context = "\n".join([r.page_content for r in results]) or "No relevant papers found."

    prompt = f"""
You are a medical research assistant specialized in Multiple Sclerosis (MS).

Use the information from the research context below to answer the user's question.
If the context is insufficient, say so and do not make up information.

Your answer should be clear, concise, and directly answer the question in paragraph form — 
do not include numbered sections, headings, or lists.

### Research Context:
{context}

### Question:
{query}

### Answer:
"""

    resp = llm.invoke(prompt)
    print("Gemini Research response raw:", resp)

    # Retry briefly if empty
    if not resp.content.strip():
        retry = llm.invoke(f"Answer briefly: {query}")
        return retry.content or "No response generated by Gemini."

    # ✅ Return plain text answer only
    return resp.content.strip()
